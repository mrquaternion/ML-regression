{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Retrieving all the libraries we will need for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "Extracting the CSV file and visualizing the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"weatherAUS.csv\")\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "Plotting the temperature in Canberra over almost a decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['Date'] = pd.to_datetime(weather['Date']) # Ensure that operations on dates can easily be done\n",
    "location = 'Canberra' # Specify the location\n",
    "filtered_weather = weather[weather['Location'] == location].loc[:, ['Date', 'Temp9am', 'Temp3pm']] # Filter the dataset by the location and only retrieve important columns\n",
    "filtered_weather['AverageTemp'] = (filtered_weather['Temp9am'] + filtered_weather['Temp3pm']) / 2 # Add average temperature during the day\n",
    "\n",
    "### Plot the new dataset ###\n",
    "plt.figure(figsize=(24, 6))\n",
    "plt.scatter(filtered_weather['Date'], filtered_weather['AverageTemp'])\n",
    "plt.title(f'Average temperature between 9AM - 3PM in {location}')\n",
    "plt.xlabel('Date (in years)')\n",
    "plt.ylabel('Temperature (in degrees celsius)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "Plotting month by temperature for the year 2015-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_weather is your dataframe and it has a 'Date' column in datetime format\n",
    "filtered_weather['Month'] = filtered_weather.Date.dt.month\n",
    "filtered_weather['Year'] = filtered_weather.Date.dt.year\n",
    "filtered_weather['Day'] = filtered_weather.Date.dt.day\n",
    "\n",
    "# Filter out unnecessary columns\n",
    "filtered_weather = filtered_weather.drop(columns=['Temp9am', 'Temp3pm'])\n",
    "\n",
    "# Define the custom date range\n",
    "start_date = \"2015-7-15\"\n",
    "end_date = \"2016-7-14\"\n",
    "\n",
    "filtered_weather = filtered_weather[(filtered_weather['Date'] >= start_date) & (filtered_weather['Date'] <= end_date)]\n",
    "\n",
    "### Plot the new dataset ###\n",
    "plt.scatter(filtered_weather.Date, filtered_weather.AverageTemp)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Temperature')\n",
    "plt.title(f'Average Temperature by Month in summer ({location})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5\n",
    "Polynomial analysis using matrix multiplication\n",
    "\n",
    "$$ X = [x^0,\\ x^1,\\ x^2]\\ \\text{where}\\ x^1\\ \\text{is the main dataset (dates)} $$\n",
    "$$ \n",
    "\\text{Coefficients of our polynomial equation}: (X^TX)^{-1}X^TY = \\begin{bmatrix}\n",
    "                    c_0 \\\\\n",
    "                    c_1 \\\\\n",
    "                    c_2\n",
    "                  \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "Y_{pred} = c_2x^2 + c_1x + c_0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg \n",
    "\n",
    "first_day = filtered_weather['Date'].iloc[0]\n",
    "filtered_weather['DateRange'] = filtered_weather.Date.map(lambda d: (d - first_day).days).astype(int)\n",
    "\n",
    "Y = filtered_weather.AverageTemp.to_numpy().reshape(-1, 1)\n",
    "X = filtered_weather.DateRange.to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_0 = np.power(X, 0)\n",
    "X_1 = X\n",
    "X_2 = np.power(X, 2)\n",
    "\n",
    "combined_X = np.hstack((X_0, X_1, X_2))\n",
    "\n",
    "coeffs = linalg.inv(combined_X.T @ combined_X) @ (combined_X.T @ Y)\n",
    "y_pred = (coeffs[2] * np.power(X, 2)) + (coeffs[1] * np.power(X, 1)) + (coeffs[0] * np.power(X, 0))\n",
    "\n",
    "### Plot the dataset with the estimated parabol ###\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.xlabel(f'Number of days since {first_day.year}-{first_day.month}-{first_day.day}')\n",
    "plt.ylabel('Average Temperature')\n",
    "plt.title(f'Average Temperature by Month in summer ({location})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "SST = np.sum(np.square(Y - np.mean(Y)))\n",
    "SSE_pred = np.sum(np.square(Y - y_pred))\n",
    "R2_pred = (1 - (SSE_pred / SST)) * 100\n",
    "print(f'The correlation between the data and the predicted polynomial regression model is {R2_pred:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our polynomial regression model fits the data, but it appears to be underfitting the highest temperatures. In the next part, we will train a model using Stochastic Gradient Descent (SGD) and regularization to find the optimal parameter that allows our regression to fit the data more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 400\n",
    "params = [0, 0.1]\n",
    "num_coeffs = 5\n",
    "\n",
    "# Normalize the datasets\n",
    "x_mean, x_std = np.mean(X), np.std(X)\n",
    "y_mean, y_std = np.mean(Y), np.std(Y)\n",
    "x_dataset = (X - x_mean) / x_std\n",
    "y_dataset = (Y - y_mean) / y_std\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_dataset, y_dataset)).batch(1)\n",
    "\n",
    "# Our polynomial model that will optimize the weights\n",
    "class PolynomialModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        weights = tf.zeros(num_coeffs, dtype=tf.float64)\n",
    "        self.w = tf.Variable(weights, dtype=tf.float64, name='parameters')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        terms = []\n",
    "        for i in range(num_coeffs):\n",
    "            term = tf.multiply(self.w[i], tf.math.pow(tf.cast(x, tf.float64), i))\n",
    "            terms.append(term)\n",
    "        \n",
    "        return tf.add_n(terms)\n",
    "    \n",
    "# Training the model\n",
    "@tf.function\n",
    "def train_step(x, y, param, optimizer, model):\n",
    "    x = tf.cast(x, tf.float64)\n",
    "    y = tf.cast(y, tf.float64)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y - y_pred)) + param * tf.reduce_sum(tf.square(model.w))\n",
    "    gradients = tape.gradient(loss, [model.w])\n",
    "    optimizer.apply_gradients(zip(gradients, [model.w]))\n",
    "    return loss\n",
    "\n",
    "#  Initialize arrays to store learned values and coefficients\n",
    "y_learned_by_param = []\n",
    "equations_coeffs = []\n",
    "\n",
    "# Iterate over different parameter values\n",
    "for param in params:\n",
    "    model = PolynomialModel()\n",
    "\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)  # Defining the optimizer\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(training_epochs):\n",
    "        for x, y in train_dataset:\n",
    "            loss = train_step(x, y, param, optimizer, model)\n",
    "\n",
    "    w_val = model.w.numpy()\n",
    "    y_learned = np.zeros_like(x_dataset, dtype=np.float64)\n",
    "    for i in range(num_coeffs):\n",
    "        y_learned += w_val[i] * np.power(x_dataset, i)\n",
    "    \n",
    "    y_learned = y_learned * y_std + y_mean # Denormalize y_learned\n",
    "\n",
    "    equations_coeffs.append(w_val) # Add separate array of the coeffs\n",
    "    y_learned_by_param.append(y_learned)\n",
    "    # Plotting the results for the current parameter\n",
    "    plt.plot(X, y_learned, label=f'Parameter = {param}')  # We do not want to plot the data with the normalized data\n",
    "\n",
    "# Adding plot information\n",
    "plt.scatter(X, Y, color='grey')  # We do not want to scatter the data with the normalized data\n",
    "plt.plot(X, y_pred, label='Original polynomial regression', color='red') # Original regression\n",
    "plt.legend()\n",
    "plt.xlabel(f'Number of days since {first_day.year}-{first_day.month}-{first_day.day}')\n",
    "plt.ylabel('Average Temperature')\n",
    "plt.title(f'Average Temperature by Month in summer ({location})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7\n",
    "Calculating the correlation between the data and plotting the residuals cloud\n",
    "$$\n",
    "Y_{learned} = c_0 + c_1x + c_2x^2 + c_3x^3 + c_4x^4 + \\varepsilon\n",
    "$$\n",
    "avec $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(params)):\n",
    "    SSE_learned = np.sum(np.square(Y - y_learned_by_param[i]))\n",
    "    R2_learned = (1 - (SSE_learned / SST)) * 100\n",
    "    print(f'The correlation between the data and the learned polynomial regression model with parameter {params[i]} is {R2_learned:.2f}%')\n",
    "\n",
    "\n",
    "residuals = y_dataset - (equations_coeffs[0][0] +\n",
    "                 equations_coeffs[0][1] * x_dataset +\n",
    "                 equations_coeffs[0][2] * np.power(x_dataset, 2) +\n",
    "                 equations_coeffs[0][3] * np.power(x_dataset, 3) +\n",
    "                 equations_coeffs[0][4] * np.power(x_dataset, 4))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_dataset, residuals)\n",
    "ax.hlines(xmin=np.min(x_dataset),xmax=np.max(x_dataset),y=0,color=\"gray\",linestyle=\"--\")\n",
    "plt.xlabel('Normalized X values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe that as the regularization parameter increases, the polynomial regression model fits the data less accurately. While experimenting with a different number of coefficients might improve the fit to some extent, it is evident that it will not fundamentally resolve the issue of the model's fitting capability. Despite these limitations, our polynomial regression model with parameter 0 still performs better in predicting the data compared to the model calculated using matrix transposition and inversion. As we can see, regularization is not always the solution if the data has already an homogeneous pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
